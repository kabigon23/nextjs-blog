---
title: 텐서플로를 사용하여 mnist 이미지 분류 모델 만들기(도서 DEEP LEARNING with Python)
date: '2023-09-26'
tags: [파이썬,인공지능,도서]
---
  
구글 colab을 활용하여 실습을 진행하였습니다.
mnist 손글씨 그림 분류하기는 전통적인 예제중 하나입니다.  
0~9까지의 손글씨 그림을 분류하는 딥러닝 모델을 구축해보겠습니다.  

밀집층을 생성하는 클래스를 정의하겠습니다.
```python
import tensorflow as tf
class NaiveDense:
    def __init__(self, input_size, output_size, activation):
    # 입력 텐서의 크기, 출력 텐서의 크기, 활성화함수를 클래스 초기화 매개변수로 받습니다.
        self.activation = activation

        w_shape = (input_size, output_size)
        # 가중치 텐서의 크기를 정합니다.
        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)
        # 가중치 텐서의 값들을 랜덤값으로 초기화합니다.
        self.W = tf.Variable(w_initial_value)
        # 가중치 텐서를 변수로 전환해줍니다.(w_initial_value는 상수라 가중치 업데이트가 안됩니다.)
        
        b_shape = (output_size,)
        # 편향 텐서의 크기를 정합니다.
        b_initial_value = tf.zeros(b_shape)
        # 편향 텐서의 값들은 0으로 초기화합니다.
        self.b = tf.Variable(b_initial_value)
        # 편향 텐서도 업데이트가 가능하도록 변수화시킵니다.

    def __call__(self, inputs):
        return self.activation(tf.matmul(inputs, self.W) + self.b)
        # 선택한 활성화 함수에 입력텐서와 가중치텐서를 점곱한 후 편향텐서를 더한 값을 인자로 넣습니다.
    
    @property
    def weights(self):
        return [self.W, self.b]
```

밀집층을 겹겹이 쌓아 연결시키는 클래스를 정의합니다.
```python
class NaiveSequential:
    def __init__(self, layers):
    # 밀집층의 배열을 매개변수로 받습니다.
        self.layers= layers

    def __call__(self, inputs):
    # 입력 텐서를 매개변수로 받으면 밀집층을 차례로 거쳐 출력하게 합니다.
        x = inputs
        for layer in self.layers:
            x = layer(x)
        return x

    @property
    def weights(self):
        weights = []
        for layer in self.layers:
            weights += layer.weights
        return weights

model = NaiveSequential([
    NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu),
    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)
]) # 활성화함수를 ReLU 함수로 갖는 입력텐서 크기 28*28, 출력텐서크기 512인 밀집층,
      활성화함수를 softmax함수로 갖는 입력텐서 크기 512, 출력텐서크기 10인 밀집층으로 구성된 배열을 입력하여 model 인스턴스를 생성합니다.
assert len(model.weights) == 4
```

전체 샘플데이터셋에서 128개의 부분데이터셋을 배치로 생성하는 클래스를 정의합니다.
```python
import math

class BatchGenerator:
    def __init__(self, images, labels, batch_size=128):
        assert len(images) == len(labels)
        self.index = 0
        self.images= images
        self.labels= labels
        self.batch_size = batch_size
        self.num_batches = math.ceil(len(images) / 128)

    def next(self):
        images = self.images[self.index:self.index+self.batch_size]
        labels = self.labels[self.index:self.index+self.batch_size]
        self.index += self.batch_size
        return images, labels
```

학습시키는 함수를 정의합니다.
```python
from tensorflow.keras import optimizers

optimizer = optimizers.SGD(learning_rate=1e-3)
# 학습의 최적화도구로 확률적 경사법을 사용합니다. learing_rate는 작은 값을 주어 가중치를 조금씩 이동하게 해야합니다.

def update_weights(gradients, weights):
    optimizer.apply_gradients(zip(gradients, weights))

def one_training_step(model, images_batch, labels_batch):
    with tf.GradientTape() as tape:
    # 미분을 위한 연산을 tape에 기록하기 위해 with...as 구문을 사용합니다.
        predictions = model(images_batch)
    # 방금 두개의 밀집층을 구축하여 만든 모델이 실행됩니다.
        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)
    # 손실함수를 사용하여 예측값과 라벨에 대한 손실을 구합니다.
        average_loss = tf.reduce_mean(per_sample_losses)
    # 평균 손실을 구합니다.
    gradients = tape.gradient(average_loss, model.weights)
    # tape에 기록된 연산 데이터를 이용하여 가중치에 대한 손실의 미분을 구합니다.
    update_weights(gradients, model.weights) # 미분값을 사용하여 가중치를 업데이트합니다.
    return average_loss
```

모델과 훈련데이터와 라벨을 입력받아 학습시키는 함수입니다.
```python
def fit(model, images, labels, epochs, batch_size=128):
    for epoch_counter in range(epochs):
    # 전체 데이터셋에 대하여 한 번 학습이 완료되는 단위를 에포크라고 합니다.
        print(f"에포크 {epoch_counter}")
        batch_generator = BatchGenerator(images, labels)
        for batch_counter in range(batch_generator.num_batches):
            images_batch, labels_batch = batch_generator.next()
            loss = one_training_step(model, images_batch, labels_batch)
            if batch_counter % 100  == 0:
                print(f"{batch_counter}번째 배치 손실:{loss:.2f}")
```

테스트를 시작합니다
```python
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
# mnist 데이터를 가져옵니다.
train_images = train_images.reshape(60000, 28*28)
train_images = train_images.astype("float32")/255
# 흑백이미지이기 때문에 한픽셀당 0~255의 값을 가집니다. 0~1의 값으로 변환을 위해 소수로 전환후 255로 나누어줍니다.
test_images = test_images.reshape(10000, 28*28)
test_images = test_images.astype("float32")/255

fit(model, train_images, train_labels, epochs=10, batch_size=128)
```
에포크 0  
0번째 배치 손실:5.14  
100번째 배치 손실:2.23  
200번째 배치 손실:2.19  
300번째 배치 손실:2.07  
400번째 배치 손실:2.26  
......  
에포크 9  
0번째 배치 손실:0.67  
100번째 배치 손실:0.70  
200번째 배치 손실:0.59  
300번째 배치 손실:0.66  
400번째 배치 손실:0.75  

에포크 9까지 학습이 진행되니 손실이 많이 줄어들었습니다.  
학습된 모델이 얼마나 정확도가 높은지 평가해보겠습니다.  
학습된 모델에 평가용 데이터를 넣은 후 출력값과 평가용 라벨을 비교합니다.
```python
import numpy as np

predictions = model(test_images)
predictions = predictions.numpy()
predicted_labels = np.argmax(predictions, axis=1)
matches = predicted_labels == test_labels
# 같으면 true(1) 다르면 false(0)을 나타냅니다.
print(f"정확도: {matches.mean():.2f}")
# 전체 true와 false의 평균을 구합니다.
```
정확도: 0.82  
82퍼센트의 정확도가 나왔습니다. 우리가 학습한 모델의 정확도는 82%라고 볼 수 있습니다.
딥러닝의 구조를 완전하고 명확하게 이해하기 위해 학습 모델을 밑바닥부터 구현해보는 의미있는 시간을 가졌습니다.
