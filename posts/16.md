---
title: 영화 리뷰 분류하기(DEEP LEARNING with Python)
date: '2023-10-03'
tags: [파이썬,코딩,인공지능,도서]
---  
영화 리뷰를 긍정과 부정으로 분류하는 모델을 구축해보겠습니다. 긍정 또는 부정 2가지를 분류하면 되기 때문에 이진 분류가 적합합니다.

## 데이터 살펴보기
IMDB(Internet Movie Database) 데이터셋을 케라스에서 가져오겠습니다. 훈련 데이터 25,000개, 테스트 데이터 25,000개, 그리고 긍정 50%, 부정 50%로 구성되어있습니다. 총 88,585개의 고유 단어가 있고 각 데이터에는 그 중 사용된 단어들이 속해있습니다. 88,585개의 단어는 불필요하게 많기 때문에 가장 자주 나타나는 단어 1만개로 제한하겠습니다.
 ```python
 from tensorflow.keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
 ```
재미삼아 첫 번째 훈련 데이터와 라벨, 그리고 텍스트로 디코딩하여 실제 포함된 영어단어들을 살펴보겠습니다.
```python
print(train_data[0])
print(train_labels[0])
```
```
[1, 14, 22, 16, 43, 530, 973, ... 178, 32] # 영어단어가 숫자로 인코딩되어 들어있습니다.
1 # 훈련 데이터 1은 긍정을 뜻합니다.
```
```python
# 자주 나타나는 단어를 1만개로 제한하였기 때문에 단어 인덱스 중 가장 높은 값은 9999이겠죠?
max([max(sequence) for squence in train_data]) # 출력값: 9999
```
첫 번째 훈련데이터가 실제 어떤 단어들로 구성되어있는지 디코딩해보겠습니다.
```python
# 단어:인덱스 맵핑 딕셔너리 가져오기
word_index = imdb.get_word_index() # key-단어 value-인덱스
# key-인덱스 value-단어가 되도록 뒤집기
reverse_word_index = dict((value, key) for (key, value) in word_index.items())
# 헷갈리겠지만 이 딕셔너리는 인덱스 0,1,2 값이 다른 값을 나타내고 있으므로 인덱스에서 3을 뺍니다. 
decoded_review = " ".join([reverse_word_index.get(i-3,"?") for i in train_data[0]])
```
## 데이터 준비
영어단어가 인코딩된 숫자로 이루어진 리스트를 신경망에 바로 입력시킬 수는 없어요. 왜냐하면 리스트의 길이가 모두 다르기 때문이에요. 신경망은 동일한 크기의 데이터를 원합니다. 그래서 리스트를 **멀티-핫 인코딩(multi-hot encoding)** 방식으로 동일한 크기인 10,000차원의 벡터로 변환할게요. 단어 인덱스가 총 1만개니까 해당 단어의 인덱스 자리엔 1, 나머지는 0으로 채우는 겁니다.
```python
import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i,j] = 1
    return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```
```python
x_train[0]
# 출력값: array([0., 1., 1., ..., 0., 0., 0.])
```
레이블은 쉽게 벡터로 바꿀 수 있습니다.
```python
y_train = np.asarray(train_labels).astype("float32")
y_test = np.asarray(test_labels).astype("float32")
```
## 신경망 모델 만들기
입력데이터는 벡터고 레이블은 0(부정) 아니면 1(긍정)의 스칼라입니다. 그럼 이제 Dense층을 쌓아 모델을 구축할게요. 
- 16개의 유닛을 가진 중간층 2개
- 입력데이터(영화 리뷰)의 0 또는 1 값의 예측을 출력하는 층 1개
   
참고로 유닛의 수를 증가시키면 표현 공간을 확장시켜 다양한 특징이나 패턴을 잡아낼 수는 있지만 과적합 위험이 증가합니다. 층의 깊이를 증가시키면 복잡한 표현 학습과 가설 공간을 확장시키지만 학습 어려움이 생길 수 있습니다.

```python
from tensorflow import keras
from tensorflow.keras import layers

# 중간층의 활성화함수 relu, 출력층의 활성화함수 sigmoid 선택
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
```
마지막으로 옵티마이저와 손실함수, 모니터링에 사용되는 지표를 선택하여 모델을 설정하겠습니다.
```python
# 옵티마이저 rmsprop, 손실함수 binary_crossentropy, 모니터링 accuracy
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
```
## 훈련 검증
딥러닝 모델은 훈련데이터에서 검증하면 안되기 때문에 기존의 훈련데이터에서 1만개의 샘플을 추출하여 검증 데이터를 만듭니다.
```python
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```
512개의 샘플씩 미니 배치를 만들고 에포크 20회를 정해서 모델을 훈련시킵니다.
```python
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```
model.fit() 메서드는 History 객체를 반환하는데 이 객체는 훈련하는 동안 발생한 정보를 담고 있습니다. 
```python
history_dict = history.history
history_dict.keys()
# 출력값: dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
```
훈련과 검증 손실 그래프를 그려볼게요.
```python
import matplotlib.pyplot as plt

loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values)+1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```
![16-1](/images/posts/16-1.png)
_훈련과 검증 손실_
이어서 훈련과 검증 정확도 그래프를 그려보겠습니다.
```python
plt.clf()
accuracy_values = history_dict["accuracy"]
val_accuracy_values = history_dict["val_accuracy"]
plt.plot(epochs, accuracy_values, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy_values, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
```
![16-2](/images/posts/16-2.png)
그래프를 보면 알 수 있듯이 훈련 손실은 에포크마다 감소하고 훈련 정확도는 에포크마다 증가합니다. 하지만 정확도는 그렇지 않습니다. 네 번째 에포크때부터 그래프가 역전되는데 **과대적합(overfitting)**이 일어나는 것입니다. 네 번째 에포크 이후부터 훈련데이터에 너무 과도하게 최적화되어 검증데이터에서 일반화되지 못한 것입니다.   
과대적합이 일어나기 전인 네번째 에포크까지 다시 훈련하고 테스트 데이터로 평가해보겠습니다.
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history = model.fit(x_train,
                    y_train,
                    epochs=4,
                    batch_size=512)
results = model.evaluate(x_test, y_test)
```
```python
results
# 출력값: [0.2834087014198303, 0.8867999911308289]
```
평가 데이터에 대한 손실이 28%, 정확도가 88%가 되었습니다.
## 새로운 데이터에 대한 예측하기
학습시킨 모델을 이용해서 평가데이터(리뷰)가 긍정인지 부정인지 예측하봅시다.
```python
model.predict(x_test)
# 출력값:
array([[0.20578116],
       [0.9990415 ],
       [0.75289613],
       ...,
       [0.09329408],
       [0.06566682],
       [0.50408506]], dtype=float32)
```
평가 데이터인 25,000개의 리뷰에 대한 긍정(1) 혹은 부정(0)의 예측을 순식간에 했습니다.
0.99이상이나 0.01이하에 대해선 확신을 갖지만 0.4~0.6 근처의 값은 확신이 부족합니다.
이것으로 영화 리뷰 분류에 대한 실습을 마치겠습니다.