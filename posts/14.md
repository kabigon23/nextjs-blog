---
title: 텐서플로 선형 분류기 구현하기(DEEP LEARNING with Python)
date: '2023-10-02'
tags: [파이썬,코딩,인공지능,도서]
---  

포스팅을 하기에 앞서 넉두리를 하자면 본격적으로 인공지능을 공부하면서 수학이 정말 중요하다는걸 계속해서 깨닫습니다. 삼십대 중반에 수학공부를 다시 하고 있을 줄은 생각을 못했네요.   

## 선형분류기란 무엇일까요?
데이터 점들을 직선으로 나누어서 분류하는 방법입니다. 예를 들어, 2차 함수 그래프 상에서 두 그룹의 점들이 있다면, 선형 분류기는 두 그룹을 구분하는 가장 적합한 직선을 찾아 그룹을 분류합니다.   

## 머신러닝을 통해 선형분류기 구현하기
그럼 머신러닝을 통해 두 그룹의 데이터 점들을 구분하는 가장 적합한 직선을 구현해볼까요?

### 합성 데이터 만들기
우선 두 그룹으로 잘 구분 되는 데이터 점들을 만들어봅시다. 특정한 평균과 공분산 행렬을 가진 랜덤 분포에서 좌표 값을 뽑아 2차원 평면의 2개의 클래스를 가지는 점들을 생성하겠습니다. 
```python
import numpy as np

# 한 그룹당 1000개의 점을 만들어서
# 2차원 평면에 두 클래스의 랜덤한 점 2000개를 생성합니다.
num_samples_per_class = 1000
negative_samples = np.random.multivariate_normal(
    mean = [0, 3],
    cov = [[1, 0.5],[0.5, 1]],
    size = num_samples_per_class
)
positive_samples = np.random.multivariate_normal(
    mean = [3, 0],
    cov = [[1, 0.5], [0.5, 1]],
    size = num_samples_per_class
)
```
```python
# 각 1000개의 점을 가진 클래스를 (2000, 2)크기의 배열로 쌓아 만듭니다.
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
```
```python
# 클래스 0과 클래스 1로 구성된 타깃을 생성합니다.
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype="float32"),
                     np.ones((num_samples_per_class, 1), dtype="float32")))

```
```python
# 두 클래스의 점들을 그래프로 그려봅시다.
import matplotlib.pyplot as plt

plt.scatter(inputs[:, 0], inputs[:, 1], c = targets[:, 0])
plt.show()
```
![14-1](/images/posts/14-1.png)
_합성데이터_

### 선형 분류기 만들기
이제 2000개의 점들을 클래스 0과 클래스 1로 구분할 수 있는 선형 분류기를 만들어보겠습니다.
선형 분류기의 방정식은 prediction = W * input + b 입니다.
머신러닝을 통한 학습을 통해 두 클래스를 구별하기에 가장 적합한 가중치 W와 b를 찾아야합니다.
예측과 타깃 사이의 차이를 제곱한 값을 최소화하도록 학습합니다.
```python
# 선형 분류기의 W와 b 변수 만들기
input_dim = 2 # 입력은 2차원 점입니다.
output_dim = 1 # 출력 예측값은 0 클래스 또는 1 클래스입니다.
W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))
b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))
```
정방향 패스함수를 만듭니다.
```python
def model(inputs):
    return tf.matmul(inputs, W) + b
```
평균 제곱 오차 손실 함수를 만듭니다.
```python
def square_loss(targets, predictions):
    per_sample_losses = tf.square(targets - predictions)
    return tf.reduce_mean(per_sample_losses)
```
데이터를 받아 이 데이터에 대한 손실을 최소화시킬 수 있는 가중치 W와 b를 찾기 위해 업데이트 합니다.
```python
def training_step(inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = square_loss(targets, predictions)
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    W.assign_sub(grad_loss_wrt_W * learning_rate)
    b.assign_sub(grad_loss_wrt_b * learning_rate)
    return loss
```
40번 에포크의 학습을 하겠습니다.
```python
for step in range(40):
    loss = training_step(inputs, targets)
    print(f"{step}번째 스텝의 손실: {loss:4f}")

```
```
0번째 스텝의 손실: 4.096818
1번째 스텝의 손실: 0.534707
......
38번째 스텝의 손실: 0.029815
39번째 스텝의 손실: 0.029412

```
학습을 반복하니 손실값이 0.02대로 줄어들었습니다.   
이 선형 모델이 훈련 데이터 점들을 어떻게 분류하는지 그려보겠습니다.
타깃은 클래스 0 또는 클래스 1이기 때문에 예측 값이 0.5보다 작으면 '0'으로 분류하고 0.5보다 크면 '1'로 분류하겠습니다.
```python
predictions = model(inputs)
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)
plt.show()
```
![14-2](/images/posts/14-2.png)
_훈련 타깃과 모델의 예측이 매우 비슷합니다._

### 선형분류기의 직선을 그려보기
2차원 평면의 직선 방정식은 w1*x + w2*y + b = 0.5가 됩니다.
이 직선보다 위에 있으면 클래스 1이고 아래에 있으면 클래스 0이 됩니다.
```python
x = np.linspace(-1, 4, 100)
y = -W[0] / W[1]*x + (0.5-b) / W[1]
plt.plot(x, y, "-r")
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:,0]>0.5)
plt.show()
```
![14-3](/images/posts/14-3.png)
클래스 0 점들과 클래스 1 점들을 분류하는 직선이 그려졌습니다.
이렇게 머신러닝을 학습을 통하여 최적의 가중치 W와 b를 구해 직선의 방정식을 찾을 수 있습니다.
